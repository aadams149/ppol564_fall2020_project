\documentclass[11pt]{article}

    \usepackage[breakable]{tcolorbox}
    \usepackage{parskip} % Stop auto-indenting (to mimic markdown behaviour)
    
    \usepackage{iftex}
    \ifPDFTeX
    	\usepackage[T1]{fontenc}
    	\usepackage{mathpazo}
    \else
    	\usepackage{fontspec}
    \fi

    % Basic figure setup, for now with no caption control since it's done
    % automatically by Pandoc (which extracts ![](path) syntax from Markdown).
    \usepackage{graphicx}
    % Maintain compatibility with old templates. Remove in nbconvert 6.0
    \let\Oldincludegraphics\includegraphics
    % Ensure that by default, figures have no caption (until we provide a
    % proper Figure object with a Caption API and a way to capture that
    % in the conversion process - todo).
    \usepackage{caption}
    \DeclareCaptionFormat{nocaption}{}
    \captionsetup{format=nocaption,aboveskip=0pt,belowskip=0pt}

    \usepackage[Export]{adjustbox} % Used to constrain images to a maximum size
    \adjustboxset{max size={0.9\linewidth}{0.9\paperheight}}
    \usepackage{float}
    \floatplacement{figure}{H} % forces figures to be placed at the correct location
    \usepackage{xcolor} % Allow colors to be defined
    \usepackage{enumerate} % Needed for markdown enumerations to work
    \usepackage{geometry} % Used to adjust the document margins
    \usepackage{amsmath} % Equations
    \usepackage{amssymb} % Equations
    \usepackage{textcomp} % defines textquotesingle
    % Hack from http://tex.stackexchange.com/a/47451/13684:
    \AtBeginDocument{%
        \def\PYZsq{\textquotesingle}% Upright quotes in Pygmentized code
    }
    \usepackage{upquote} % Upright quotes for verbatim code
    \usepackage{eurosym} % defines \euro
    \usepackage[mathletters]{ucs} % Extended unicode (utf-8) support
    \usepackage{fancyvrb} % verbatim replacement that allows latex
    \usepackage{grffile} % extends the file name processing of package graphics 
                         % to support a larger range
    \makeatletter % fix for grffile with XeLaTeX
    \def\Gread@@xetex#1{%
      \IfFileExists{"\Gin@base".bb}%
      {\Gread@eps{\Gin@base.bb}}%
      {\Gread@@xetex@aux#1}%
    }
    \makeatother

    % The hyperref package gives us a pdf with properly built
    % internal navigation ('pdf bookmarks' for the table of contents,
    % internal cross-reference links, web links for URLs, etc.)
    \usepackage{hyperref}
    % The default LaTeX title has an obnoxious amount of whitespace. By default,
    % titling removes some of it. It also provides customization options.
    \usepackage{titling}
    \usepackage{longtable} % longtable support required by pandoc >1.10
    \usepackage{booktabs}  % table support for pandoc > 1.12.2
    \usepackage[inline]{enumitem} % IRkernel/repr support (it uses the enumerate* environment)
    \usepackage[normalem]{ulem} % ulem is needed to support strikethroughs (\sout)
                                % normalem makes italics be italics, not underlines
    \usepackage{mathrsfs}
    

    
    % Colors for the hyperref package
    \definecolor{urlcolor}{rgb}{0,.145,.698}
    \definecolor{linkcolor}{rgb}{.71,0.21,0.01}
    \definecolor{citecolor}{rgb}{.12,.54,.11}

    % ANSI colors
    \definecolor{ansi-black}{HTML}{3E424D}
    \definecolor{ansi-black-intense}{HTML}{282C36}
    \definecolor{ansi-red}{HTML}{E75C58}
    \definecolor{ansi-red-intense}{HTML}{B22B31}
    \definecolor{ansi-green}{HTML}{00A250}
    \definecolor{ansi-green-intense}{HTML}{007427}
    \definecolor{ansi-yellow}{HTML}{DDB62B}
    \definecolor{ansi-yellow-intense}{HTML}{B27D12}
    \definecolor{ansi-blue}{HTML}{208FFB}
    \definecolor{ansi-blue-intense}{HTML}{0065CA}
    \definecolor{ansi-magenta}{HTML}{D160C4}
    \definecolor{ansi-magenta-intense}{HTML}{A03196}
    \definecolor{ansi-cyan}{HTML}{60C6C8}
    \definecolor{ansi-cyan-intense}{HTML}{258F8F}
    \definecolor{ansi-white}{HTML}{C5C1B4}
    \definecolor{ansi-white-intense}{HTML}{A1A6B2}
    \definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
    \definecolor{ansi-default-inverse-bg}{HTML}{000000}

    % commands and environments needed by pandoc snippets
    % extracted from the output of `pandoc -s`
    \providecommand{\tightlist}{%
      \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
    \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
    % Add ',fontsize=\small' for more characters per line
    \newenvironment{Shaded}{}{}
    \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.56,0.13,0.00}{{#1}}}
    \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.25,0.63,0.44}{{#1}}}
    \newcommand{\CharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\StringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textit{{#1}}}}
    \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{{#1}}}
    \newcommand{\AlertTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.02,0.16,0.49}{{#1}}}
    \newcommand{\RegionMarkerTok}[1]{{#1}}
    \newcommand{\ErrorTok}[1]{\textcolor[rgb]{1.00,0.00,0.00}{\textbf{{#1}}}}
    \newcommand{\NormalTok}[1]{{#1}}
    
    % Additional commands for more recent versions of Pandoc
    \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.53,0.00,0.00}{{#1}}}
    \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.25,0.44,0.63}{{#1}}}
    \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.73,0.40,0.53}{{#1}}}
    \newcommand{\ImportTok}[1]{{#1}}
    \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.73,0.13,0.13}{\textit{{#1}}}}
    \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.10,0.09,0.49}{{#1}}}
    \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.44,0.13}{\textbf{{#1}}}}
    \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.40,0.40,0.40}{{#1}}}
    \newcommand{\BuiltInTok}[1]{{#1}}
    \newcommand{\ExtensionTok}[1]{{#1}}
    \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.74,0.48,0.00}{{#1}}}
    \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.49,0.56,0.16}{{#1}}}
    \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.38,0.63,0.69}{\textbf{\textit{{#1}}}}}
    
    
    % Define a nice break command that doesn't care if a line doesn't already
    % exist.
    \def\br{\hspace*{\fill} \\* }
    % Math Jax compatibility definitions
    \def\gt{>}
    \def\lt{<}
    \let\Oldtex\TeX
    \let\Oldlatex\LaTeX
    \renewcommand{\TeX}{\textrm{\Oldtex}}
    \renewcommand{\LaTeX}{\textrm{\Oldlatex}}
    % Document parameters
    % Document title
    \title{adams\_alexander\_final\_report}
    
    
    
    
    
% Pygments definitions
\makeatletter
\def\PY@reset{\let\PY@it=\relax \let\PY@bf=\relax%
    \let\PY@ul=\relax \let\PY@tc=\relax%
    \let\PY@bc=\relax \let\PY@ff=\relax}
\def\PY@tok#1{\csname PY@tok@#1\endcsname}
\def\PY@toks#1+{\ifx\relax#1\empty\else%
    \PY@tok{#1}\expandafter\PY@toks\fi}
\def\PY@do#1{\PY@bc{\PY@tc{\PY@ul{%
    \PY@it{\PY@bf{\PY@ff{#1}}}}}}}
\def\PY#1#2{\PY@reset\PY@toks#1+\relax+\PY@do{#2}}

\expandafter\def\csname PY@tok@w\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.73,0.73}{##1}}}
\expandafter\def\csname PY@tok@c\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.74,0.48,0.00}{##1}}}
\expandafter\def\csname PY@tok@k\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.69,0.00,0.25}{##1}}}
\expandafter\def\csname PY@tok@o\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ow\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@nb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@nn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@ne\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.82,0.25,0.23}{##1}}}
\expandafter\def\csname PY@tok@nv\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@no\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@nl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@ni\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.60,0.60,0.60}{##1}}}
\expandafter\def\csname PY@tok@na\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.49,0.56,0.16}{##1}}}
\expandafter\def\csname PY@tok@nt\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@nd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.67,0.13,1.00}{##1}}}
\expandafter\def\csname PY@tok@s\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sd\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@si\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@se\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.13}{##1}}}
\expandafter\def\csname PY@tok@sr\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.40,0.53}{##1}}}
\expandafter\def\csname PY@tok@ss\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sx\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@m\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@gh\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gu\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.50,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@gd\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.63,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@gi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.63,0.00}{##1}}}
\expandafter\def\csname PY@tok@gr\endcsname{\def\PY@tc##1{\textcolor[rgb]{1.00,0.00,0.00}{##1}}}
\expandafter\def\csname PY@tok@ge\endcsname{\let\PY@it=\textit}
\expandafter\def\csname PY@tok@gs\endcsname{\let\PY@bf=\textbf}
\expandafter\def\csname PY@tok@gp\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,0.50}{##1}}}
\expandafter\def\csname PY@tok@go\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.53,0.53,0.53}{##1}}}
\expandafter\def\csname PY@tok@gt\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.27,0.87}{##1}}}
\expandafter\def\csname PY@tok@err\endcsname{\def\PY@bc##1{\setlength{\fboxsep}{0pt}\fcolorbox[rgb]{1.00,0.00,0.00}{1,1,1}{\strut ##1}}}
\expandafter\def\csname PY@tok@kc\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kd\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kn\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@kr\endcsname{\let\PY@bf=\textbf\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@bp\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.50,0.00}{##1}}}
\expandafter\def\csname PY@tok@fm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.00,0.00,1.00}{##1}}}
\expandafter\def\csname PY@tok@vc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vg\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@vm\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.10,0.09,0.49}{##1}}}
\expandafter\def\csname PY@tok@sa\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sc\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@dl\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s2\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@sh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@s1\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.73,0.13,0.13}{##1}}}
\expandafter\def\csname PY@tok@mb\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mf\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mh\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mi\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@il\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@mo\endcsname{\def\PY@tc##1{\textcolor[rgb]{0.40,0.40,0.40}{##1}}}
\expandafter\def\csname PY@tok@ch\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cm\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cpf\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@c1\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}
\expandafter\def\csname PY@tok@cs\endcsname{\let\PY@it=\textit\def\PY@tc##1{\textcolor[rgb]{0.25,0.50,0.50}{##1}}}

\def\PYZbs{\char`\\}
\def\PYZus{\char`\_}
\def\PYZob{\char`\{}
\def\PYZcb{\char`\}}
\def\PYZca{\char`\^}
\def\PYZam{\char`\&}
\def\PYZlt{\char`\<}
\def\PYZgt{\char`\>}
\def\PYZsh{\char`\#}
\def\PYZpc{\char`\%}
\def\PYZdl{\char`\$}
\def\PYZhy{\char`\-}
\def\PYZsq{\char`\'}
\def\PYZdq{\char`\"}
\def\PYZti{\char`\~}
% for compatibility with earlier versions
\def\PYZat{@}
\def\PYZlb{[}
\def\PYZrb{]}
\makeatother


    % For linebreaks inside Verbatim environment from package fancyvrb. 
    \makeatletter
        \newbox\Wrappedcontinuationbox 
        \newbox\Wrappedvisiblespacebox 
        \newcommand*\Wrappedvisiblespace {\textcolor{red}{\textvisiblespace}} 
        \newcommand*\Wrappedcontinuationsymbol {\textcolor{red}{\llap{\tiny$\m@th\hookrightarrow$}}} 
        \newcommand*\Wrappedcontinuationindent {3ex } 
        \newcommand*\Wrappedafterbreak {\kern\Wrappedcontinuationindent\copy\Wrappedcontinuationbox} 
        % Take advantage of the already applied Pygments mark-up to insert 
        % potential linebreaks for TeX processing. 
        %        {, <, #, %, $, ' and ": go to next line. 
        %        _, }, ^, &, >, - and ~: stay at end of broken line. 
        % Use of \textquotesingle for straight quote. 
        \newcommand*\Wrappedbreaksatspecials {% 
            \def\PYGZus{\discretionary{\char`\_}{\Wrappedafterbreak}{\char`\_}}% 
            \def\PYGZob{\discretionary{}{\Wrappedafterbreak\char`\{}{\char`\{}}% 
            \def\PYGZcb{\discretionary{\char`\}}{\Wrappedafterbreak}{\char`\}}}% 
            \def\PYGZca{\discretionary{\char`\^}{\Wrappedafterbreak}{\char`\^}}% 
            \def\PYGZam{\discretionary{\char`\&}{\Wrappedafterbreak}{\char`\&}}% 
            \def\PYGZlt{\discretionary{}{\Wrappedafterbreak\char`\<}{\char`\<}}% 
            \def\PYGZgt{\discretionary{\char`\>}{\Wrappedafterbreak}{\char`\>}}% 
            \def\PYGZsh{\discretionary{}{\Wrappedafterbreak\char`\#}{\char`\#}}% 
            \def\PYGZpc{\discretionary{}{\Wrappedafterbreak\char`\%}{\char`\%}}% 
            \def\PYGZdl{\discretionary{}{\Wrappedafterbreak\char`\$}{\char`\$}}% 
            \def\PYGZhy{\discretionary{\char`\-}{\Wrappedafterbreak}{\char`\-}}% 
            \def\PYGZsq{\discretionary{}{\Wrappedafterbreak\textquotesingle}{\textquotesingle}}% 
            \def\PYGZdq{\discretionary{}{\Wrappedafterbreak\char`\"}{\char`\"}}% 
            \def\PYGZti{\discretionary{\char`\~}{\Wrappedafterbreak}{\char`\~}}% 
        } 
        % Some characters . , ; ? ! / are not pygmentized. 
        % This macro makes them "active" and they will insert potential linebreaks 
        \newcommand*\Wrappedbreaksatpunct {% 
            \lccode`\~`\.\lowercase{\def~}{\discretionary{\hbox{\char`\.}}{\Wrappedafterbreak}{\hbox{\char`\.}}}% 
            \lccode`\~`\,\lowercase{\def~}{\discretionary{\hbox{\char`\,}}{\Wrappedafterbreak}{\hbox{\char`\,}}}% 
            \lccode`\~`\;\lowercase{\def~}{\discretionary{\hbox{\char`\;}}{\Wrappedafterbreak}{\hbox{\char`\;}}}% 
            \lccode`\~`\:\lowercase{\def~}{\discretionary{\hbox{\char`\:}}{\Wrappedafterbreak}{\hbox{\char`\:}}}% 
            \lccode`\~`\?\lowercase{\def~}{\discretionary{\hbox{\char`\?}}{\Wrappedafterbreak}{\hbox{\char`\?}}}% 
            \lccode`\~`\!\lowercase{\def~}{\discretionary{\hbox{\char`\!}}{\Wrappedafterbreak}{\hbox{\char`\!}}}% 
            \lccode`\~`\/\lowercase{\def~}{\discretionary{\hbox{\char`\/}}{\Wrappedafterbreak}{\hbox{\char`\/}}}% 
            \catcode`\.\active
            \catcode`\,\active 
            \catcode`\;\active
            \catcode`\:\active
            \catcode`\?\active
            \catcode`\!\active
            \catcode`\/\active 
            \lccode`\~`\~ 	
        }
    \makeatother

    \let\OriginalVerbatim=\Verbatim
    \makeatletter
    \renewcommand{\Verbatim}[1][1]{%
        %\parskip\z@skip
        \sbox\Wrappedcontinuationbox {\Wrappedcontinuationsymbol}%
        \sbox\Wrappedvisiblespacebox {\FV@SetupFont\Wrappedvisiblespace}%
        \def\FancyVerbFormatLine ##1{\hsize\linewidth
            \vtop{\raggedright\hyphenpenalty\z@\exhyphenpenalty\z@
                \doublehyphendemerits\z@\finalhyphendemerits\z@
                \strut ##1\strut}%
        }%
        % If the linebreak is at a space, the latter will be displayed as visible
        % space at end of first line, and a continuation symbol starts next line.
        % Stretch/shrink are however usually zero for typewriter font.
        \def\FV@Space {%
            \nobreak\hskip\z@ plus\fontdimen3\font minus\fontdimen4\font
            \discretionary{\copy\Wrappedvisiblespacebox}{\Wrappedafterbreak}
            {\kern\fontdimen2\font}%
        }%
        
        % Allow breaks at special characters using \PYG... macros.
        \Wrappedbreaksatspecials
        % Breaks at punctuation characters . , ; ? ! and / need catcode=\active 	
        \OriginalVerbatim[#1,codes*=\Wrappedbreaksatpunct]%
    }
    \makeatother

    % Exact colors from NB
    \definecolor{incolor}{HTML}{303F9F}
    \definecolor{outcolor}{HTML}{D84315}
    \definecolor{cellborder}{HTML}{CFCFCF}
    \definecolor{cellbackground}{HTML}{F7F7F7}
    
    % prompt
    \makeatletter
    \newcommand{\boxspacing}{\kern\kvtcb@left@rule\kern\kvtcb@boxsep}
    \makeatother
    \newcommand{\prompt}[4]{
        \ttfamily\llap{{\color{#2}[#3]:\hspace{3pt}#4}}\vspace{-\baselineskip}
    }
    

    
    % Prevent overflowing lines due to hard-to-break entities
    \sloppy 
    % Setup hyperref package
    \hypersetup{
      breaklinks=true,  % so long urls are correctly broken across lines
      colorlinks=true,
      urlcolor=urlcolor,
      linkcolor=linkcolor,
      citecolor=citecolor,
      }
    % Slightly bigger margins than the latex defaults
    
    \geometry{verbose,tmargin=1in,bmargin=1in,lmargin=1in,rmargin=1in}
    
    

\begin{document}
    
    \maketitle
    
    

    
    \begin{tcolorbox}[breakable, size=fbox, boxrule=1pt, pad at break*=1mm,colback=cellbackground, colframe=cellborder]
\prompt{In}{incolor}{1}{\boxspacing}
\begin{Verbatim}[commandchars=\\\{\}]
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}NOTE: I found that rendering a Jupyter Notebook into a PDF through LaTeX includes the code with no way to remove it.}
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}To get around this, I am writing my report in this notebook (along with notes where images and footnotes and citations}
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}should go), and then I am going to export this as a .tex file and load it into OverLeaf (an online LaTeX editor) so that}
\PY{c+c1}{\PYZsh{}\PYZsh{}\PYZsh{}I can polish up the final PDF. This notebook is as much a workspace as it is a final product.}
\end{Verbatim}
\end{tcolorbox}

    \hypertarget{alexander-adams}{%
\section{Alexander Adams}\label{alexander-adams}}

\hypertarget{ppol564-data-science-1-foundations}{%
\section{PPOL564 Data Science 1:
Foundations}\label{ppol564-data-science-1-foundations}}

\hypertarget{final-project}{%
\section{Final Project}\label{final-project}}

\hypertarget{december-17-2020}{%
\section{December 17, 2020}\label{december-17-2020}}

    \hypertarget{abstract}{%
\subsubsection{Abstract}\label{abstract}}

-- Student clearly established the aim of the project.

The aim of this project is to assess the effectiveness of various
factors in predicting which of the two major party candidates (Democrat
Hillary Clinton and Republican Donald Trump) won more votes in a given
county in the 2016 United States presidential election. The purpose of
this report is to discuss the motivations for this project, the data
gathering and cleaning process, analyze and present the results of the
most effective machine learning methods, and consider possible next
steps for this research topic. This project also attempts to draw
conclusions from misclassified observations data, and briefly discusses
potential real-world implications of the machine learning results.

    \hypertarget{problem-statement-and-background}{%
\subsubsection{Problem Statement and
Background}\label{problem-statement-and-background}}

The outcome of the 2016 presidential election came as a surprise to a
large number of voters and observers in the United States and around the
world. Many (myself included) expected Hillary Clinton to defeat Donald
Trump, a prediction based off of polling and forecasting from various
media outlets. While Clinton won the popular vote, Trump won the
electoral college, and thus the presidency. After the election,
political analysts, particularly on the left, felt it necessary to
figure out what went wrong, and to learn from 2016. The focus of this
project can be stated thusly: 1) what factors predict the major-party
candidate who received more votes in a given county in the 2016 U.S.
presidential election, and 2) are there any notable patterns present
among incorrectly predicted counties?

    -- Student included a brief summary of any related work (i.e.~a light
literature review)

In the course of writing this report, I found it useful to frame the
literature review in terms of relevance to specific aspects of this
project. First, I consider including data on voting outcomes in previous
presidential elections. Over the past few decades, the process of
partisan sorting among American voters has progressed significantly.
Politically liberal Americans now almost exclusively claim membership in
the Democratic party, while their conservative peers are
near-exclusively Republicans. (Fiorina Citation) Race also plays a
significant factor regarding vote choice in the U.S. Nonwhite voters
regardless of race tend to vote for Democratic candidates (FOOTNOTE:
While there exist differences between nationalities within broader
racial categories (such as Cuban Americans versus Mexican Americans, or
Vietnamese Americans versus Chinese Americans), such differences are
beyond the scope of this inquiry.); while white voters are more divided,
the Republican candidate usually wins a plurality of those votes.
Nonwhite voter turnout, particularly among Black voters, increased from
2004 to 2008 and 2008 to 2012. However, the gap between white and
nonwhite voter turnout increased from 10\% in 2012 to 12.6\% in 2016,
with Black voters turning out at a lower rate than any election since
2000.(BROOKINGS Citation). Many voters practice economic voting, or
choosing who to vote for based on economic issues. The literature on
economic voting, specifically as it pertains to unemployment, is
inconclusive; some of the literature suggests that the Democratic party
benefits when unemployment is high, since that party is perceived as
being stronger on economic issues, while other research suggests a more
general negative effect on the incumbent party. (REEVES AND PARK
citation) In 2016, the Democratic party was the incumbent party, so it
is unclear if unemployment produces an effect on vote choice.

    \hypertarget{data-sources-and-processing}{%
\subsubsection{Data Sources and
Processing}\label{data-sources-and-processing}}

-- Student outlined where their data came from.

The data set used in this project consists of 3,139 observations of 114
variables (1 outcome variable and 113 feature variables). The unit of
observation for this data set is the county or county-equivalent level.
I specify county-equivalent because Alaska is divided into boroughs and
borough-equivalent census areas, Louisiana is divided into parishes, and
some cities are formally incorporated as independent cities and are
considered equivalent to counties. Each county or county-equivalent has
a unique five-digit Federal Information Processing Standards (FIPS)
code. The 50 states and the District of Columbia collectively encompass
3,142 FIPS codes; the three codes not included in the data are all in
Alaska, and were dropped due to high levels of missingness (including
the dependent variable).

    One issue with this research question arises from the way the United
States administers elections. The combination of winner-take-all
elections (with the exceptions of Nebraska and Maine) and the electoral
college system means that votes are effectively allocated at the state
level. Because of this, county level data is thus limited in the
information it can provide about winning elections (since someone can
win most of the counties in a state but still lose the state, depending
on how populations are distributed). However, a county-level data set
can contain over 3,000 observations, while a state level data set can
only contain 51 (including the District of Columbia). Even if I used my
initial unit of analysis (state-year) and pooled data from election
cycles going back to 1976, I would still end up with a total (italics) n
that is less than 20\% of the county-level data set. This small data set
would then be split into training and test data, meaning the model would
be trained on an even smaller number of observations. I would expect any
model I use to overfit the data and have limited predictive power on the
test data.

    I gathered my data from several sources. The data on unemployment came
from the U.S. Department of Agriculture. The data on demographics (race,
gender, and age) came from the U.S. Census Bureau. The data on vote
totals and outcomes came from the MIT Election Lab. The data on vote
totals and outcomes for the state of Alaska came from rrhelections.com,
an explicitly Republican elections website. While using data from a
partisan source is not ideal, I have not observed any quality issues in
the data. (FOOTNOTE: Alaska engages in a curious practice regarding its
election results. Rather than report votes by county or
county-equivalent, it reports by state legislative district. There are
40 state house districts and 30 county-equivalents, and their respective
boundaries do not meaningfully overlap. Since the other data I had was
at the county level, I needed to find a source for Alaska vote data at
that geographical unit. rrhelections.com took the data for each election
and constructed county level vote shares and totals for the major party
candidates for all presidential elections from 1960 to the present. They
presented this data as a series of .pngs of data tables, so I had to
manually type the data into a spreadsheet to convert it to a usable
form. However, considering that the alternative was to ignore the state
of Alaska in my analysis, I feel satisfied in making this choice.) The
data on marriages and divorces was gathered by researchers at Bowling
Green State University, and the data on health and social indicators was
gathered by researchers at the County Health Rankings Project run by the
University of Wisconsin Population Health Institute.

    The dependent variable for this analysis is a dichotomous variable which
codes whether Clinton or Trump received more votes in a county in 2016.
This variable is coded as 1 if Clinton won more votes, and 0 if Trump
won more votes. Given that the overwhelming majority of votes cast in
2016 were for one of the two major-party candidates, I elected to ignore
third parties in this analysis.

    The 113 feature variables used for this project encompass race, gender,
and age (i.e.~the proportion of 18-45 year-olds who are white men, the
proportion of adults older than 65 who are black women, etc) in 2016,
unemployment levels and workforce size from 2000 to 2016, level of
urbanization as of 2013, health metrics, and major party vote shares in
previous elections, as well as a small number of miscellaneous variables
included purely out of curiosity. These miscellaneous variables include
the proportion of a county population which is not proficient in
english, the proportion with some college education, the proportion of
people who drive alone to work with a regular commute lasting 30 minutes
or longer, and the number of marriages and divorces in that county in
2010. (FOOTNOTE: For a complete list of outcome and feature variables,
see
ppol564\_fall2020\_project/Project\_Files/Codebooks/aja149\_cleandata\_revised\_csv\_codebook.rmd.)

    Rightly or wrongly, I tried to preserve as many counties in my data as
possible, and only dropped those where I could not find data for the
dependent variable (as I mention in my discussion of Alaska above). In
one case, this required data to be more or less fabricated. Broomfield
County, Colorado (FIPS code 08014) was formed from parts of Adams,
Boulder, Jefferson, and Weld counties in 2001. As such, it did not exist
in the election data for the 2000 election. To solve this issue, I
averaged the voting results for the four counties which formed
Broomfield and inserted it into the data set at the appropriate index.
Similarly, I replaced the Alaska observations (at the state legislative
district level) included in the raw data set with county level data to
maintain continuity across different data sources. For each county for a
given year, I added the total votes earned by the Democratic candidate
and the Republican candidate to find the total two-party vote, then
calculated proportions of that vote for both parties.

    The raw demographic data I used contained almost 750,000 rows (the unit
of analysis for that raw data was county-year-age group). The size of
this data set hindered my ability to process it, so I dropped all
observations for years other than 2016 and recoded age from 18
categories to 3. To process age, I dropped all observations containing
data for age groups unable to vote. Since one age group spanned 15-19
year olds, the youngest individuals reflected in my data set are 20
years old. I recoded the data into categories of 20-45, 45-65, and 65+,
resulting in columns for the number of each racial category and gender
for a given age (e.g.~the number of Asian women aged 45-65). To
standardize these numbers, I converted them to proportions of their age
group (e.g.~the proportion of 20-45 year olds who are white women). The
choice of age categories was intended to reflect standard
classifications roughly corresponding to young adults, middle-aged
adults, and senior citizens.

    Overall, the data used for this project did not demonstrate a
significant degree of missingness (no variable had more than 2-3\% of
its observations missing).

    \hypertarget{analysis}{%
\subsubsection{Analysis}\label{analysis}}

-- Student described the methods/tools they explored in their project.

This analysis required the use of a number of machine learning tools
found within the python package scikit-learn (often referred to as
sklearn). Once I was satisfied with the quality of my data set, I split
in two ways: first, into a single-variable outcome data set (``Y'') and
a multi-variable feature data set (``X''), and second, into a
``training'' set comprised of 75\% of the observations in X and Y and a
``test'' set containing the remaining 25\%. The split into training and
test data requires the specification of a random state; since the data
can be randomly divided an extremely large number of ways, specifying a
random state means that the same division will be chosen every time,
allowing results to be replicated. (FOOTNOTE: For this analysis, I
specified random state = 149 for the training-test split.) This method
allows machine learning models to be evaluated on their performance
based on unfamiliar data, to gain a more complete understanding of the
model's accuracy and predictive ability. After converting the data into
training and test sets, the next step was to fill in any missing values
with the mean of that variable. Doing this after the training-test split
ensures that the test data is not in any way biased or affected by the
training data.

    The next step was to specify a k-fold generator. K-fold cross-validation
is a machine learning technique which splits up a data set into a
specified number of subsets, then processes each subset except for one
(the ``validation'' subset) according to the specified algorithm. An
error rate can be calculated for each tested subset, and then the
average error rate becomes the cross-validation score. Like the
training-test split, k-fold cross-validation also requires the
specification of a random state to ensure that replication is possible.
(FOOTNOTE: For the k-fold cross-validation for this analysis, random
state = 298.)

    Having created training and test data sets and specified a k-fold
generator, the next step in this analysis was to scale all numerical
variables using the preprocessing function MinMaxScaler. This function
scales a numeric variable so that all values are between 0 and 1. It is
especially useful in cases like this, where some variables reflect
county populations which can vary significantly.

    Finally, I specified the algorithms I wanted to test. For this analysis
I used classifiers (machine learning tools which predict which one of a
group of predetermined categories best describes a value of a discrete
outcome variable) rather than regressors (which predict the value of a
continuous outcome variable). I tested four algorithms for this
analysis. The first is a naive bayes classifier. A naive bayes
classifier calculates the probability of a given observation belonging
to a particular class based on the assumption that all feature variables
are independent of each other. The second algorithm is a
k-nearest-neighbors classifier. For a given observation a, this
classifier calculates the k observations closest to a and classifies a
according to which class represents the plurality of the k observations
(i.e.~for a given observation a which can be ``red'' or ``blue'', if k
equals five, the classifier finds the five points closest to a. If three
of those are red and two are blue, the classifier predicts that a is
red.). The third algorithm is a decision tree classifier. Decision trees
work by identifying splits in a feature variable which sort observations
into branching categories. For example, a hypothetical decision tree
based off of the data for this project might first divide the data based
on whether unemployment levels in 2016 were above or below a certain
threshold. Each of those divisions would then be divided further using
other feature variables until the tree reaches a terminal state,
typically specified by either a specific number of branches or a
proportion of the data remaining in a split (splits in decision trees
and related algorithms are called nodes). Finally, I tested a random
forest classifier. Random forest algorithms operate under the assumption
that different feature variables hold different levels of predictive
power in a decision tree. As such, a random forest algorithm ``grows'' a
specified number of decision trees, each one using a random set of
feature variables (to ensure that highly predictive variables do not
obscure the effects of other variables) and a specific number of nodes.
These trees are then averaged to result in a final set of predictions.

    The sklearn package includes a function called GridSearch, which allows
for multiple algorithms with multiple parameter specifications to be run
using a single command. sklearn also enables the creation of a machine
learning pipeline, which incorporates the k-fold generator, the
MinMaxScaler processing function, and the GridSearch algorithm tester in
one function. Since this analysis is centered on classification, I chose
to evaluate the accuracy of each algorithm using the ROC-AUC statistic,
which compares the rate of true positives to false positives.

    Furthermore, the sklearn package contains functions to facilitate the
creation of partial dependency plots of the most important variables for
each tested set of feature variables. These plots indicate the
relationships between a given feature variable and the outcome variable
in the context of a machine learning algorithm. I also used the package
plotly to create maps of the counties included in my test data, to
visualize correct and incorrect predictions.

    \hypertarget{results}{%
\subsubsection{Results}\label{results}}

-- Student gave a detailed summary of their results.

I tested each machine learning algorithm with different combinations of
feature variables.The first combination I tested included all 113
feature variables in my data set. Running the GridSearch returned a
maximum AUC score of 0.9848, generated by the random forest algorithm.
This score was obtained before I tuned any of the parameters for any of
the models. (FOOTNOTE: Ultimately, I chose not to tune any parameters
for this project for two reasons: first, the initial accuracy achieved
with a default set of parameters was over 95\%, meaning that additional
tuning efforts would almost certainly have diminishing marginal returns,
and second, the overall goal of this project is to identify patterns in
the errors, meaning that the quality of my results are not dependent on
having a perfectly tuned model.) Since the highest possible value for an
AUC score is 1, I was curious as to which feature variables could be
responsible for such a high score. To find this information, I tested
the permutation importance of the feature variables. The top 5 most
important variables in descending order of importance were 1) the
proportion of the two-party vote earned by the democratic candidate for
president in 2012, 2) the proportion of the two-party vote earned by the
democratic candidate for president in 2008, 3) the proportion of the
two-party vote earned by the democratic candidate for president in 2004,
4) the proportion of the two-party vote earned by the democratic
candidate for president in 2000, and 5) the proportion of 18-45 year
olds who are Asian-American women. Roughly 50 of the feature variables
were reported as having a permutation importance of 0, meaning that they
did not affect the accuracy of the model.

    (INSERT pdependplot\_model1test.png)

    Figure 1 shows the partial dependency plots for the eight most important
variables in this set of feature variables (democratic vote share in the
previous four elections, proportions of 20-45 year old Asian women and
men, and proportions of 45-65 year old Asian women and men). There is a
clear visual difference in the vote share partial dependencies as
compared to the demographic partial dependencies: the former are very
steep, indicating that they exert a high degree of influence on the
predicted probabilities (and thus potentially obscure other effects or
relationships).

    Having run the classifier, it was then possible to generate predicted
outcomes for each county.

    (INSERT model1\_testmap\_final.png)

    Figure 2 shows the counties or county-equivalents in the test data.
Counties shaded red were correctly predicted as Trump victories, those
shaded in blue were correctly predicted as Clinton victories, and those
shaded in green were incorrectly predicted for Trump but were really won
by Clinton. 4.45\% of the counties in the test data (35 out of 785) were
misclassified; all 35 were predicted to be won by Trump. Interestingly,
the classifier did a particularly poor job at predicting the winning
candidate in Vermont. There are 14 counties in Vermont, 7 of which were
sorted into the test data. 6 of those 7 were incorrectly predicted for
Trump. The exception was Chittenden County (FIPS code 50007), the most
populous county in the state (roughly one in four Vermont residents
lives in Chittenden County). Table 1 shows the predictions for the
Vermont counties in the test data under a model specification which
includes all feature variables. Also interesting was the dispersal of
the misclassified counties: of the 48 states represented in the training
data, 19 had at least one misclassified county. Only Vermont (6),
Colorado (4), and North Carolina (3) had more than 2 errors.

    (SEE TABLE 1)

\begin{tabular}{rlllrr}
\toprule
 index &   FIPS & Stabr &              area\_name &  D\_win2016 &  pred \\
\midrule
  2810 &  50015 &    VT &    Lamoille County, VT &        1.0 &   0.0 \\
  2809 &  50013 &    VT &  Grand Isle County, VT &        1.0 &   0.0 \\
  2806 &  50007 &    VT &  Chittenden County, VT &        1.0 &   1.0 \\
  2804 &  50003 &    VT &  Bennington County, VT &        1.0 &   0.0 \\
  2813 &  50021 &    VT &     Rutland County, VT &        1.0 &   0.0 \\
  2816 &  50027 &    VT &     Windsor County, VT &        1.0 &   0.0 \\
  2811 &  50017 &    VT &      Orange County, VT &        1.0 &   0.0 \\
\bottomrule
\end{tabular}

    On the one hand, it makes sense that past voting behavior would be more
effective at predicting future outcomes than other factors, given the
degree to which partisan sorting in the United States has progressed
over the past several decades. However, ``partisanship in the United
States is so static that the biggest predictor of which party will win a
county is the party that won it four years ago'' is a somewhat
unsatisfying conclusion. As such, I decided to rerun the GridSearch,
excluding the feature variables related to past voting outcomes.
(FOOTNOTE: These include the proportion of the two-party vote share won
by the democrat in 2000,2004,2008,and 2012, the total number of votes
cast in each of those elections, and the two-party vote total in those
elections). The random forest algorithm again produced the highest AUC
score of 0.9262 (as with the first iteration, this is without any
significant attempt to tune the model). The most important variables for
this specification were 1) the proportion of 20-45 year olds who are
Asian men, 2) the proportion of 20-45 year olds who are Asian women, 3)
the proportion of adults who have completed some college, 4) the
proportion of adults older than 65 who are Asian men, and 5) the
proportion of adults older than 65 who are white men. Figure 3 shows the
partial dependency plots for the eight most important feature variables
given this specification. (INSERT pdepend\_model2test.png).

This model had an error rate of 7.26\%, which represents an increase in
error of 63\% over the algorithm trained on the voting data from
previous elections. (FOOTNOTE: In real terms, this means the algorithm
misclassified 57 out of the 785 counties in the training data.)

Figure 4 shows the counties included in the test data for this set of
feature variables. (INSERT model2\_testmap\_final.png)

As with the previous specification, the most successful algorithm
trained on this data was much more likely to misclassify a county in
favor of Trump than of Clinton, with 54 out of 57 erroneous
classifications incorrectly predicting Trump would win that county. The
three counties in the test data which the algorithm incorrectly called
for Clinton are Denton County, Texas, Duval County, Florida, and
Monmouth County, New Jersey.

I expect that this favorability toward Trump results from the real
distribution of counties: Clinton may have won more votes, but Trump won
more counties (three or four for every one won by Clinton). The modal
outcome is a Trump victory, and so it is not wholly surprising that a
machine learning algorithm would be more likely to incorrectly predict a
Trump victory than incorrectly predict a Clinton victory. In the
aftermath of the election, much attention was given to three particular
states considered to be pivotal in securing victory for Trump: Michigan,
Pennsylvania, and Wisconsin. Figures 2 and 4 show that of the counties
within these states which were included in the test data, only 2 or 3
counties (depending on the selected feature variables) were incorrectly
classified; all of these were incorrectly predicted for Trump, and all
are in Wisconsin. Future investigations focusing on predicting vote
totals or vote shares, rather than which candidate won a plurality of
votes, could determine if this misclassification could significantly
impact a prediction of this election.

Overall, the number of misclassified counties is too small and
geographically disparate to draw strong conclusions, but Figures 2 and 4
show misclassifications in the southwest, particularly Arizona, New
Mexico, and south Texas. Considering how New Mexico and Arizona both
voted Democratic overall in 2020, with Arizona electing two Democratic
senators in the cycles following 2016, this suggests that the Southwest
could be emerging as a political power center for the Democratic party,
similar to how the great plains states are a power center for the
Republican party.

    \hypertarget{discussion}{%
\subsubsection{Discussion}\label{discussion}}

-- Student spoke on the ``success'' of their project (as defined in
their proposal). âˆ— ``Did you achieve what you set out to do? If not
why?''

I reviewed my proposal to assess the success of this project, and have
concluded that I did not achieve what I set out to do in my proposal.
However, this requires clarification: the aim of my project (and the
data I used) changed significantly between the time I wrote the proposal
and the time I conducted this analysis and wrote this report. I
initially planned to use data at the state-year level, but later
switched to the county level (and focused on 2016 in particular) to
increase the size of my data set. I ultimately ended up using data from
a wider range of sources than I listed in my proposal. The decision to
switch to county-level data and to focus on classification rather than
regression allowed me to create more interesting visualizations and
observe more interesting trends than if I had used state-level data. My
initial proposal also focused specifically on a link between partisan
vote share (as the outcome variable) and unemployment rate, while the
actual project focuses on which major-party candidate wins a given
jurisdiction (and unemployment rate becomes one of a large number of
feature variables, rather than the sole variable of interest). Perverse
though it may sound, in considering this project I am glad to have
``failed'' in my original aims.

    -- Student articulate how they would expand the analysis if given more
time.

The primary way in which I would expand my analysis is to incorporate
more data on education levels. I was able to find data on the proportion
of a county population with some college education, but I was not able
to find data on other levels of educational attainment at the county
level without significant degrees of missingness. Additionally, I would
have liked to more thoroughly investigate the misclassified counties for
the two sets of feature variables used. While I was able to create maps
of which counties were classified in particular ways, I did not have
time to search for patterns in the erroneously classified counties
beyond basic geography. Finally, given more time, I would like to test
regressions on this data in addition to classifiers. Doing this would
provide insight into how inaccurate these classifications were, and
could potentially open up other lines of inquiry depending on which
variables are most important in the regression.


    % Add a bibliography block to the postdoc
    
    
    
\end{document}
