{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "###NOTE: I found that rendering a Jupyter Notebook into a PDF through LaTeX includes the code with no way to remove it.\n",
    "###To get around this, I am writing my report in this notebook (along with notes where images and footnotes and citations\n",
    "###should go), and then I am going to export this as a .tex file and load it into OverLeaf (an online LaTeX editor) so that\n",
    "###I can polish up the final PDF. This notebook is as much a workspace as it is a final product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alexander Adams\n",
    "\n",
    "# PPOL564 Data Science 1: Foundations\n",
    "\n",
    "# Final Project\n",
    "\n",
    "# December 17, 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "– Student clearly established the aim of the project. \n",
    "\n",
    "The aim of this project is to assess the effectiveness of various factors in predicting which of the two major party candidates (Democrat Hillary Clinton and Republican Donald Trump) won more votes in a given county in the 2016 United States presidential election. The purpose of this report is to discuss the motivations for this project, the data gathering and cleaning process, analyze and present the results of the most effective machine learning methods, and consider possible next steps for this research topic. This project also attempts to draw conclusions from misclassified observations data, and briefly discusses potential real-world implications of the machine learning results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem Statement and Background\n",
    "\n",
    "The outcome of the 2016 presidential election came as a surprise to a large number of voters and observers in the United States and around the world. Many (myself included) expected Hillary Clinton to defeat Donald Trump, a prediction based off of polling and forecasting from various media outlets. While Clinton won the popular vote, Trump won the electoral college, and thus the presidency. After the election, political analysts, particularly on the left, felt it necessary to figure out what went wrong, and to learn from 2016. The focus of this project can be stated thusly: 1) what factors predict the major-party candidate who received more votes in a given county in the 2016 U.S. presidential election, and 2) are there any notable patterns present among incorrectly predicted counties? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "– Student included a brief summary of any related work (i.e. a light literature review) \n",
    "\n",
    "In the course of writing this report, I found it useful to frame the literature review in terms of relevance to specific aspects of this project. First, I consider including data on voting outcomes in previous presidential elections. Over the past few decades, the process of partisan sorting among American voters has progressed significantly. Politically liberal Americans now almost exclusively claim membership in the Democratic party, while their conservative peers are near-exclusively Republicans. (Fiorina Citation) Race also plays a significant factor regarding vote choice in the U.S. Nonwhite voters regardless of race tend to vote for Democratic candidates (FOOTNOTE: While there exist differences between nationalities within broader racial categories (such as Cuban Americans versus Mexican Americans, or Vietnamese Americans versus Chinese Americans), such differences are beyond the scope of this inquiry.); while white voters are more divided, the Republican candidate usually wins a plurality of those votes. Nonwhite voter turnout, particularly among Black voters, increased from 2004 to 2008 and 2008 to 2012. However, the gap between white and nonwhite voter turnout increased from 10% in 2012 to 12.6% in 2016, with Black voters turning out at a lower rate than any election since 2000.(BROOKINGS Citation). Many voters practice economic voting, or choosing who to vote for based on economic issues. The literature on economic voting, specifically as it pertains to unemployment, is inconclusive; some of the literature suggests that the Democratic party benefits when unemployment is high, since that party is perceived as being stronger on economic issues, while other research suggests a more general negative effect on the incumbent party. (REEVES AND PARK citation) In 2016, the Democratic party was the incumbent party, so it is unclear if unemployment produces an effect on vote choice.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sources and Processing \n",
    "\n",
    "– Student outlined where their data came from. \n",
    "\n",
    "The data set used in this project consists of 3,139 observations of 114 variables (1 outcome variable and 113 feature variables). The unit of observation for this data set is the county or county-equivalent level. I specify county-equivalent because Alaska is divided into boroughs and borough-equivalent census areas, Louisiana is divided into parishes, and some cities are formally incorporated as independent cities and are considered equivalent to counties. Each county or county-equivalent has a unique five-digit Federal Information Processing Standards (FIPS) code. The 50 states and the District of Columbia collectively encompass 3,142 FIPS codes; the three codes not included in the data are all in Alaska, and were dropped due to high levels of missingness (including the dependent variable)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue with this research question arises from the way the United States administers elections. The combination of winner-take-all elections (with the exceptions of Nebraska and Maine) and the electoral college system means that votes are effectively allocated at the state level. Because of this, county level data is thus limited in the information it can provide about winning elections (since someone can win most of the counties in a state but still lose the state, depending on how populations are distributed). However, a county-level data set can contain over 3,000 observations, while a state level data set can only contain 51 (including the District of Columbia). Even if I used my initial unit of analysis (state-year) and pooled data from election cycles going back to 1976, I would still end up with a total (italics) n that is less than 20% of the county-level data set. This small data set would then be split into training and test data, meaning the model would be trained on an even smaller number of observations. I would expect any model I use to overfit the data and have limited predictive power on the test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I gathered my data from several sources. The data on unemployment came from the U.S. Department of Agriculture. The data on demographics (race, gender, and age) came from the U.S. Census Bureau. The data on vote totals and outcomes came from the MIT Election Lab. The data on vote totals and outcomes for the state of Alaska came from rrhelections.com, an explicitly Republican elections website. While using data from a partisan source is not ideal, I have not observed any quality issues in the data. (FOOTNOTE: Alaska engages in a curious practice regarding its election results. Rather than report votes by county or county-equivalent, it reports by state legislative district. There are 40 state house districts and 30 county-equivalents, and their respective boundaries do not meaningfully overlap. Since the other data I had was at the county level, I needed to find a source for Alaska vote data at that geographical unit. rrhelections.com took the data for each election and constructed county level vote shares and totals for the major party candidates for all presidential elections from 1960 to the present. They presented this data as a series of .pngs of data tables, so I had to manually type the data into a spreadsheet to convert it to a usable form. However, considering that the alternative was to ignore the state of Alaska in my analysis, I feel satisfied in making this choice.) The data on marriages and divorces was gathered by researchers at Bowling Green State University, and the data on health and social indicators was gathered by researchers at the County Health Rankings Project run by the University of Wisconsin Population Health Institute.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dependent variable for this analysis is a dichotomous variable which codes whether Clinton or Trump received more votes in a county in 2016. This variable is coded as 1 if Clinton won more votes, and 0 if Trump won more votes. Given that the overwhelming majority of votes cast in 2016 were for one of the two major-party candidates, I elected to ignore third parties in this analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 113 feature variables used for this project encompass race, gender, and age (i.e. the proportion of 18-45 year-olds who are white men, the proportion of adults older than 65 who are black women, etc) in 2016, unemployment levels and workforce size from 2000 to 2016, level of urbanization as of 2013, health metrics, and major party vote shares in previous elections, as well as a small number of miscellaneous variables included purely out of curiosity. These miscellaneous variables include the proportion of a county population which is not proficient in english, the proportion with some college education, the proportion of people who drive alone to work with a regular commute lasting 30 minutes or longer, and the number of marriages and divorces in that county in 2010. (FOOTNOTE: For a complete list of outcome and feature variables, see ppol564_fall2020_project/Project_Files/Codebooks/aja149_cleandata_revised_csv_codebook.rmd.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rightly or wrongly, I tried to preserve as many counties in my data as possible, and only dropped those where I could not find data for the dependent variable (as I mention in my discussion of Alaska above). In one case, this required data to be more or less fabricated. Broomfield County, Colorado (FIPS code 08014) was formed from parts of Adams, Boulder, Jefferson, and Weld counties in 2001. As such, it did not exist in the election data for the 2000 election. To solve this issue, I averaged the voting results for the four counties which formed Broomfield and inserted it into the data set at the appropriate index. Similarly, I replaced the Alaska observations (at the state legislative district level) included in the raw data set with county level data to maintain continuity across different data sources. For each county for a given year, I added the total votes earned by the Democratic candidate and the Republican candidate to find the total two-party vote, then calculated proportions of that vote for both parties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw demographic data I used contained almost 750,000 rows (the unit of analysis for that raw data was county-year-age group). The size of this data set hindered my ability to process it, so I dropped all observations for years other than 2016 and recoded age from 18 categories to 3. To process age, I dropped all observations containing data for age groups unable to vote. Since one age group spanned 15-19 year olds, the youngest individuals reflected in my data set are 20 years old. I recoded the data into categories of 20-45, 45-65, and 65+, resulting in columns for the number of each racial category and gender for a given age (e.g. the number of Asian women aged 45-65). To standardize these numbers, I converted them to proportions of their age group (e.g. the proportion of 20-45 year olds who are white women). The choice of age categories was intended to reflect standard classifications roughly corresponding to young adults, middle-aged adults, and senior citizens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, the data used for this project did not demonstrate a significant degree of missingness (no variable had more than 2-3% of its observations missing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "– Student described the methods/tools they explored in their project. \n",
    "\n",
    "This analysis required the use of a number of machine learning tools found within the python package scikit-learn (often referred to as sklearn). Once I was satisfied with the quality of my data set, I split in two ways: first, into a single-variable outcome data set (\"Y\") and a multi-variable feature data set (\"X\"), and second, into a \"training\" set comprised of 75% of the observations in X and Y and a \"test\" set containing the remaining 25%. The split into training and test data requires the specification of a random state; since the data can be randomly divided an extremely large number of ways, specifying a random state means that the same division will be chosen every time, allowing results to be replicated. (FOOTNOTE: For this analysis, I specified random state = 149 for the training-test split.) This method allows machine learning models to be evaluated on their performance based on unfamiliar data, to gain a more complete understanding of the model's accuracy and predictive ability. After converting the data into training and test sets, the next step was to fill in any missing values with the mean of that variable. Doing this after the training-test split ensures that the test data is not in any way biased or affected by the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step was to specify a k-fold generator. K-fold cross-validation is a machine learning technique which splits up a data set into a specified number of subsets, then processes each subset except for one (the \"validation\" subset) according to the specified algorithm. An error rate can be calculated for each tested subset, and then the average error rate becomes the cross-validation score. Like the training-test split, k-fold cross-validation also requires the specification of a random state to ensure that replication is possible. (FOOTNOTE: For the k-fold cross-validation for this analysis, random state = 298.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having created training and test data sets and specified a k-fold generator, the next step in this analysis was to scale all numerical variables using the preprocessing function MinMaxScaler. This function scales a numeric variable so that all values are between 0 and 1. It is especially useful in cases like this, where some variables reflect county populations which can vary significantly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I specified the algorithms I wanted to test. For this analysis I used classifiers (machine learning tools which predict which one of a group of predetermined categories best describes a value of a discrete outcome variable) rather than regressors (which predict the value of a continuous outcome variable). I tested four algorithms for this analysis. The first is a naive bayes classifier. A naive bayes classifier calculates the probability of a given observation belonging to a particular class based on the assumption that all feature variables are independent of each other. The second algorithm is a k-nearest-neighbors classifier. For a given observation a, this classifier calculates the k observations closest to a and classifies a according to which class represents the plurality of the k observations (i.e. for a given observation a which can be \"red\" or \"blue\", if k equals five, the classifier finds the five points closest to a. If three of those are red and two are blue, the classifier predicts that a is red.). The third algorithm is a decision tree classifier. Decision trees work by identifying splits in a feature variable which sort observations into branching categories. For example, a hypothetical decision tree based off of the data for this project might first divide the data based on whether unemployment levels in 2016 were above or below a certain threshold. Each of those divisions would then be divided further using other feature variables until the tree reaches a terminal state, typically specified by either a specific number of branches or a proportion of the data remaining in a split (splits in decision trees and related algorithms are called nodes). Finally, I tested a random forest classifier. Random forest algorithms operate under the assumption that different feature variables hold different levels of predictive power in a decision tree. As such, a random forest algorithm \"grows\" a specified number of decision trees, each one using a random set of feature variables (to ensure that highly predictive variables do not obscure the effects of other variables) and a specific number of nodes. These trees are then averaged to result in a final set of predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sklearn package includes a function called GridSearch, which allows for multiple algorithms with multiple parameter specifications to be run using a single command. sklearn also enables the creation of a machine learning pipeline, which incorporates the k-fold generator, the MinMaxScaler processing function, and the GridSearch algorithm tester in one function. Since this analysis is centered on classification, I chose to evaluate the accuracy of each algorithm using the ROC-AUC statistic, which compares the rate of true positives to false positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, the sklearn package contains functions to facilitate the creation of partial dependency plots of the most important variables for each tested set of feature variables. These plots indicate the relationships between a given feature variable and the outcome variable in the context of a machine learning algorithm. I also used the package plotly to create maps of the counties included in my test data, to visualize correct and incorrect predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "– Student gave a detailed summary of their results.\n",
    "\n",
    "I tested each machine learning algorithm with different combinations of feature variables.The first combination I tested included all 113 feature variables in my data set. Running the GridSearch returned a maximum AUC score of 0.9848, generated by the random forest algorithm. This score was obtained before I tuned any of the parameters for any of the models. (FOOTNOTE: Ultimately, I chose not to tune any parameters for this project for two reasons: first, the initial accuracy achieved with a default set of parameters was over 95%, meaning that additional tuning efforts would almost certainly have diminishing marginal returns, and second, the overall goal of this project is to identify patterns in the errors, meaning that the quality of my results are not dependent on having a perfectly tuned model.) Since the highest possible value for an AUC score is 1, I was curious as to which feature variables could be responsible for such a high score. To find this information, I tested the permutation importance of the feature variables. The top 5 most important variables in descending order of importance were 1) the proportion of the two-party vote earned by the democratic candidate for president in 2012, 2) the proportion of the two-party vote earned by the democratic candidate for president in 2008, 3) the proportion of the two-party vote earned by the democratic candidate for president in 2004, 4) the proportion of the two-party vote earned by the democratic candidate for president in 2000, and 5) the proportion of 18-45 year olds who are Asian-American women. Roughly 50 of the feature variables were reported as having a permutation importance of 0, meaning that they did not affect the accuracy of the model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(INSERT pdependplot_model1test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 1 shows the partial dependency plots for the eight most important variables in this set of feature variables (democratic vote share in the previous four elections, proportions of 20-45 year old Asian women and men, and proportions of 45-65 year old Asian women and men). There is a clear visual difference in the vote share partial dependencies as compared to the demographic partial dependencies: the former are very steep, indicating that they exert a high degree of influence on the predicted probabilities (and thus potentially obscure other effects or relationships)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having run the classifier, it was then possible to generate predicted outcomes for each county.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(INSERT model1_testmap_final.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure 2 shows the counties or county-equivalents in the test data. Counties shaded red were correctly predicted as Trump victories, those shaded in blue were correctly predicted as Clinton victories, and those shaded in green were incorrectly predicted for Trump but were really won by Clinton. 4.45% of the counties in the test data (35 out of 785) were misclassified; all 35 were predicted to be won by Trump. Interestingly, the classifier did a particularly poor job at predicting the winning candidate in Vermont. There are 14 counties in Vermont, 7 of which were sorted into the test data. 6 of those 7 were incorrectly predicted for Trump. The exception was Chittenden County (FIPS code 50007), the most populous county in the state (roughly one in four Vermont residents lives in Chittenden County). Table 1 shows the predictions for the Vermont counties in the test data under a model specification which includes all feature variables. Also interesting was the dispersal of the misclassified counties: of the 48 states represented in the training data, 19 had at least one misclassified county. Only Vermont (6), Colorado (4), and North Carolina (3) had more than 2 errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(SEE TABLE 1) \n",
    "\n",
    "\\begin{tabular}{rlllrr}\n",
    "\\toprule\n",
    " index &   FIPS & Stabr &              area\\_name &  D\\_win2016 &  pred \\\\\n",
    "\\midrule\n",
    "  2810 &  50015 &    VT &    Lamoille County, VT &        1.0 &   0.0 \\\\\n",
    "  2809 &  50013 &    VT &  Grand Isle County, VT &        1.0 &   0.0 \\\\\n",
    "  2806 &  50007 &    VT &  Chittenden County, VT &        1.0 &   1.0 \\\\\n",
    "  2804 &  50003 &    VT &  Bennington County, VT &        1.0 &   0.0 \\\\\n",
    "  2813 &  50021 &    VT &     Rutland County, VT &        1.0 &   0.0 \\\\\n",
    "  2816 &  50027 &    VT &     Windsor County, VT &        1.0 &   0.0 \\\\\n",
    "  2811 &  50017 &    VT &      Orange County, VT &        1.0 &   0.0 \\\\\n",
    "\\bottomrule\n",
    "\\end{tabular}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the one hand, it makes sense that past voting behavior would be more effective at predicting future outcomes than other factors, given the degree to which partisan sorting in the United States has progressed over the past several decades. However, “partisanship in the United States is so static that the biggest predictor of which party will win a county is the party that won it four years ago” is a somewhat unsatisfying conclusion. As such, I decided to rerun the GridSearch, excluding the feature variables related to past voting outcomes. (FOOTNOTE: These include the proportion of the two-party vote share won by the democrat in 2000,2004,2008,and 2012, the total number of votes cast in each of those elections, and the two-party vote total in those elections). The random forest algorithm again produced the highest AUC score of 0.9262 (as with the first iteration, this is without any significant attempt to tune the model). The most important variables for this specification were 1) the proportion of 20-45 year olds who are Asian men, 2) the proportion of 20-45 year olds who are Asian women, 3) the proportion of adults who have completed some college, 4) the proportion of adults older than 65 who are Asian men, and 5) the proportion of adults older than 65 who are white men. Figure 3 shows the partial dependency plots for the eight most important feature variables given this specification. (INSERT pdepend_model2test.png). \n",
    "\n",
    "This model had an error rate of 7.26%, which represents an increase in error of 63% over the algorithm trained on the voting data from previous elections. (FOOTNOTE: In real terms, this means the algorithm misclassified 57 out of the 785 counties in the training data.) \n",
    "\n",
    "Figure 4 shows the counties included in the test data for this set of feature variables. (INSERT model2_testmap_final.png) \n",
    "\n",
    "As with the previous specification, the most successful algorithm trained on this data was much more likely to misclassify a county in favor of Trump than of Clinton, with 54 out of 57 erroneous classifications incorrectly predicting Trump would win that county. The three counties in the test data which the algorithm incorrectly called for Clinton are Denton County, Texas, Duval County, Florida, and Monmouth County, New Jersey.\n",
    "\n",
    "I expect that this favorability toward Trump results from the real distribution of counties: Clinton may have won more votes, but Trump won more counties (three or four for every one won by Clinton). The modal outcome is a Trump victory, and so it is not wholly surprising that a machine learning algorithm would be more likely to incorrectly predict a Trump victory than incorrectly predict a Clinton victory.\n",
    "In the aftermath of the election, much attention was given to three particular states considered to be pivotal in securing victory for Trump: Michigan, Pennsylvania, and Wisconsin. Figures 2 and 4 show that of the counties within these states which were included in the test data, only 2 or 3 counties (depending on the selected feature variables) were incorrectly classified; all of these were incorrectly predicted for Trump, and all are in Wisconsin. Future investigations focusing on predicting vote totals or vote shares, rather than which candidate won a plurality of votes, could determine if this misclassification could significantly impact a prediction of this election.\n",
    "\n",
    "Overall, the number of misclassified counties is too small and geographically disparate to draw strong conclusions, but Figures 2 and 4 show misclassifications in the southwest, particularly Arizona, New Mexico, and south Texas. Considering how New Mexico and Arizona both voted Democratic overall in 2020, with Arizona electing two Democratic senators in the cycles following 2016, this suggests that the Southwest could be emerging as a political power center for the Democratic party, similar to how the great plains states are a power center for the Republican party. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "– Student spoke on the “success” of their project (as defined in their proposal). ∗ “Did you achieve what you set out to do? If not why?”\n",
    "\n",
    "I reviewed my proposal to assess the success of this project, and have concluded that I did not achieve what I set out to do in my proposal. However, this requires clarification: the aim of my project (and the data I used) changed significantly between the time I wrote the proposal and the time I conducted this analysis and wrote this report. I initially planned to use data at the state-year level, but later switched to the county level (and focused on 2016 in particular) to increase the size of my data set. I ultimately ended up using data from a wider range of sources than I listed in my proposal. The decision to switch to county-level data and to focus on classification rather than regression allowed me to create more interesting visualizations  and observe more interesting trends than if I had used state-level data. My initial proposal also focused specifically on a link between partisan vote share (as the outcome variable) and unemployment rate, while the actual project focuses on which major-party candidate wins a given jurisdiction (and unemployment rate becomes one of a large number of feature variables, rather than the sole variable of interest). Perverse though it may sound, in considering this project I am glad to have \"failed\" in my original aims.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "– Student articulate how they would expand the analysis if given more time.\n",
    "\n",
    "The primary way in which I would expand my analysis is to incorporate more data on education levels. I was able to find data on the proportion of a county population with some college education, but I was not able to find data on other levels of educational attainment at the county level without significant degrees of missingness. Additionally, I would have liked to more thoroughly investigate the misclassified counties for the two sets of feature variables used. While I was able to create maps of which counties were classified in particular ways, I did not have time to search for patterns in the erroneously classified counties beyond basic geography. Finally, given more time, I would like to test regressions on this data in addition to classifiers. Doing this would provide insight into how inaccurate these classifications were, and could potentially open up other lines of inquiry depending on which variables are most important in the regression."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
